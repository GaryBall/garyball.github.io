<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Andreea Bobu | Publications</title>
  <meta name="description" content="A beautiful Jekyll theme for academics">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Andreea</span> Bobu</a>
     	<div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center" style="line-height: 1em;">
            <a href="mailto:abobu@berkeley.edu"><i class="fa fa-envelope-square gm-icon"></i></a>
            <a href="https://scholar.google.com/citations?user=62e5CygAAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar-square gs-icon"></i></a>
            <a href="https://github.com/andreea7b" target="_blank" title="GitHub"><i class="fab fa-github-square gh-icon"></i></a>
            <a href="https://twitter.com/andreea7b" target="_blank" title="Twitter"><i class="fab fa-twitter-square tw-icon"></i></a>
          </span>
        </div>
 
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
              <li class="nav-item ">
                  <a class="nav-link" href="/assets/pdf/vitae.pdf">
                    Curriculum Vitae
                  </a>
              </li>
           
		          <!-- 
              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    Projects
                    
                  </a>
              </li>
            	-->

              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/publications/">
                    Publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
              </li>
            
              <li class="nav-item ">
                  <a class="nav-link" href="/teaching/">
                    Teaching
                    
                  </a>
              </li>

              <li class="nav-item ">
                  <a class="nav-link" href="/service/">
                    Service
                                        
                  </a>
              </li>
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Publications</h1>
  <h6><nobr><em>*</em></nobr> denotes equal contribution and joint lead authorship.</h6>


<p><br /></p>


<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
	

		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://arxiv.org/" target="_blank">
          arXiv
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2022ICRA" class="col p-0">
      <h5 class="title mb-0">Learning Perceptual Concepts by Bootstrapping from Human Queries</h5>
      <div class="author">
                
		
							<nobr><em>Andreea Bobu</em>,</nobr>              
                
                  <nobr><a href="https://cpaxton.github.io/about/" target="_blank">Chris Paxton</a>,</nobr>

                  <nobr><a href="https://research.nvidia.com/person/wei-yang" target="_blank">Wei Yang</a>,</nobr>

                  <nobr><a href="https://balakumar-s.github.io/" target="_blank">Balakumar Sundaralingam</a>,</nobr>

                  <nobr><a href="https://research.nvidia.com/person/yuwei-chao" target="_blank">Yu-Wei Chao</a>,</nobr>

                  <nobr><a href="https://homes.cs.washington.edu/~mcakmak/" target="_blank">Maya Cakmak</a>,</nobr>

									and
                
                  <nobr><a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
        ArXiv Preprint, 2022
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2022ICRA-abstract" role="button" aria-expanded="false" aria-controls="bobu2022ICRA-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/NVIDIA/bobu2022ICRA.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/nvidia.com/active-concept-learning" target="_blank">Project Website</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="bobu2022ICRA-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
	Robots need to be able to learn concepts from their users in order to adapt their capabilities to each user’s unique task. But when the robot operates on high-dimensional inputs, like images or point clouds, this is impractical: the robot needs an unrealistic amount of human effort to learn the new concept. To address this challenge, we propose a new approach whereby the robot learns a low-dimensional variant of the concept and uses it to generate a larger data set for learning the concept in the high-dimensional space.  This lets it take advantage of semantically meaningful privileged information only accessible at training time, like object poses and bounding boxes, that allows for richer human interaction to speed up learning. We evaluate our approach by learning prepositional concepts that describe object state or multi-object relationships, like above, near, or aligned, which are key to user specification of task goals and execution constraints for robots. Using a simulated human, we show that our approach improves sample complexity when compared to learning concepts directly in the high-dimensional space. We also demonstrate the utility of the learned concepts in motion planning tasks on a 7-DoF Franka robot.
	</div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>



	<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://journals.sagepub.com/home/ijr" target="_blank">
          IJRR
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2022IJRR" class="col p-0">
      <h5 class="title mb-0">Inducing Structure in Reward Learning by Learning Features</h5>
      <div class="author">
                
									<nobr><em>Andreea Bobu</em>,</nobr>
              
                  <nobr><a href="https://www.linkedin.com/in/marius-wiggert/" target="_blank">Marius Wiggert<nobr></nobr></a>,</nobr>
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~tomlin/" target="_blank">Claire J. Tomlin</a>,</nobr>
              
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          The International Journal of Robotics Research
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2022IJRR-abstract" role="button" aria-expanded="false" aria-controls="bobu2022IJRR-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/FERL/bobu2022IJRR.pdf" target="_blank">PDF</a>
          
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="bobu2022IJRR-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
	Reward learning enables robots to learn adaptable behaviors from human input. Traditional methods model the reward as a linear function of hand-crafted features, but that requires specifying all the relevant features a priori, which is impossible for real-world tasks. To get around this issue, recent deep Inverse Reinforcement Learning (IRL) methods learn rewards directly from the raw state but this is challenging because the robot has to implicitly learn the features that are important and how to combine them, simultaneously. Instead, we propose a divide and conquer approach: focus human input specifically on learning the features separately, and only then learn how to combine them into a reward. We introduce a novel type of human input for teaching features and an algorithm that utilizes it to learn complex features from the raw state space. The robot can then learn how to combine them into a reward using demonstrations, corrections, or other reward learning frameworks. We demonstrate our method in settings where all features have to be learned from scratch, as well as where some of the features are known. By first focusing human input specifically on the feature(s), our method decreases sample complexity and improves generalization of the learned reward over a deepIRL baseline. We show this in experiments with a physical 7DOF robot manipulator, as well as in a user study conducted in a simulated environment.
	</div>
        </div>
      </div>
      
    </div>
  </div>
</div>
			
</li></ol>

			
			
			</ol>
    </div>
  </div>





<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2021</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
	

		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://ras.papercept.net/conferences/conferences/ICRA21/program/ICRA21_ProgramAtAGlanceWeb.html" target="_blank">
          ICRA
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="zurek2021ICRA" class="col p-0">
      <h5 class="title mb-0">Situational Confidence Assistance for Lifelong Shared Autonomy</h5>
      <div class="author">
                
                  <nobr>Matthew Zurek<nobr><em>*</em></nobr></a>,</nobr>
		
							<nobr><em>Andreea Bobu*</em>,</nobr>              
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~dsbrown/" target="_blank">Daniel S. Brown</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
        International Conference on Robot Learning (ICRA), 2021
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#zurek2021ICRA-abstract" role="button" aria-expanded="false" aria-controls="zurek2021ICRA-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/CASA/zurek2021ICRA.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/A4k3B3uewBs?t=8337" target="_blank">Talk</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="zurek2021ICRA-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
	Shared autonomy enables robots to infer user intent and assist in accomplishing it. But when the user wants to do a new task that the robot does not know about, shared autonomy will hinder their performance by attempting to assist them with something that is not their intent. Our key idea is that the robot can detect when its repertoire of intents is insufficient to explain the user's input, and give them back control. This then enables the robot to observe unhindered task execution, learn the new intent behind it, and add it to this repertoire. We demonstrate with both a case study and a user study that our proposed method maintains good performance when the human's intent is in the robot's repertoire, outperforms prior shared autonomy approaches when it isn't, and successfully learns new skills, enabling efficient lifelong learning for confidence-based shared autonomy.          
</div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>


		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://ras.papercept.net/conferences/conferences/ICRA21/program/ICRA21_ProgramAtAGlanceWeb.html" target="_blank">
          ICRA
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="sripathy2021ICRA" class="col p-0">
      <h5 class="title mb-0">Dynamically Switching Human Prediction Models for Efficient Planning</h5>
      <div class="author">
                
                  <nobr><a href="https://www.linkedin.com/in/arjunsripathy" target="_blank">Arjun Sripathy<nobr><em>*</em></nobr></a>,</nobr>
		
							<nobr><em>Andreea Bobu*</em>,</nobr>              
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~dsbrown/" target="_blank">Daniel S. Brown</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
        International Conference on Robot Learning (ICRA), 2021
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#sripathy2021ICRA-abstract" role="button" aria-expanded="false" aria-controls="sripathy2021ICRA-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/modelswitching/sripathy2021ICRA.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arjunsripathy.github.io/model_switching/" target="_blank">Project Website</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/arjunsripathy/model_switching" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=g1x-gQYYSag&t=14s&ab_channel=ArjunSripathy" target="_blank">Talk</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="sripathy2021ICRA-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
		As environments involving both robots and humans become increasingly common, so does the need to account for people during planning. To plan effectively, robots must be able to respond to and sometimes influence what humans do. This requires a human model which predicts future human actions. A simple model may assume the human will continue what they did previously; a more complex one might predict that the human will act optimally, disregarding the robot; whereas an even more complex one might capture the robot's ability to influence the human. These models make different trade-offs between computational time and performance of the resulting robot plan. Using only one model of the human either wastes computational resources or is unable to handle critical situations. In this work, we give the robot access to a suite of human models and enable it to assess the performance-computation trade-off online. By estimating how an alternate model could improve human prediction and how that may translate to performance gain, the robot can dynamically switch human models whenever the additional computation is justified. Our experiments in a driving simulator showcase how the robot can achieve performance comparable to always using the best human model, but with greatly reduced computation.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>





	<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://humanrobotinteraction.org/2021/" target="_blank">
          HRI
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2021HRI" class="col p-0">
      <h5 class="title mb-0">Feature Expansive Reward Learning: Rethinking Human Input</h5>
      <div class="author">
                
									<nobr><em>Andreea Bobu*</em>,</nobr>
              
                  <nobr><a href="https://www.linkedin.com/in/marius-wiggert/" target="_blank">Marius Wiggert<nobr><em>*</em></nobr></a>,</nobr>
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~tomlin/" target="_blank">Claire J. Tomlin</a>,</nobr>
              
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          ACM/IEEE International Conference on Human Robot Interaction (HRI), 2021
          <br>
        <nobr><em style="color:orange;">Best Paper Award Finalist</em></nobr>
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2021HRI-abstract" role="button" aria-expanded="false" aria-controls="bobu2021HRI-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/FERL/bobu2021HRI.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/view/feature-learning" target="_blank">Project Website</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/andreea7b/FERL" target="_blank">Code</a>
	  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=NaD-eVri8r4&ab_channel=InterACTLab" target="_blank">Talk</a>
          
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="bobu2021HRI-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
        When a person is not satisfied with how a robot performs a task, they can intervene to correct it. Reward learning methods enable the robot to adapt its reward function online based on such human input, but they rely on handcrafted features. When the correction cannot be explained by these features, recent work in deep Inverse Reinforcement Learning (IRL) suggests that the robot could ask for task demonstrations and recover a reward defined over the raw state space. Our insight is that rather than implicitly learning about the missing feature(s) from demonstrations, the robot should instead ask for data that explicitly teaches it about what it is missing. We introduce a new type of human input in which the person guides the robot from states where the feature being taught is highly expressed to states where it is not. We propose an algorithm for learning the feature from the raw state space and integrating it into the reward function. By focusing the human input on the missing feature, our method decreases sample complexity and improves generalization of the learned reward over the above deep IRL baseline. We show this in experiments with a physical 7DOF robot manipulator, as well as in a user study conducted in a simulated environment.  
	</div>
        </div>
      </div>
      
    </div>
  </div>
</div>
			
</li></ol>

			
			
			</ol>
    </div>
  </div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2020</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://humanrobotinteraction.org/2020/" target="_blank">
          HRI
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2020HRI" class="col p-0">
      <h5 class="title mb-0">LESS is More: Rethinking Probabilistic Models of Human Behavior</h5>
      <div class="author">
                
									<nobr><em>Andreea Bobu*</em>,</nobr>
              
                  <nobr><a href="https://www.linkedin.com/in/dexterscobee/" target="_blank">Dexter R. R. Scobee<nobr><em>*</em></nobr></a>,</nobr>
                
                  <nobr><a href="https://ece.princeton.edu/people/jaime-fernandez-fisac" target="_blank">Jaime F. Fisac</a>,</nobr>
            
                  <nobr><a href="https://people.eecs.berkeley.edu/~sastry/" target="_blank">S. Shankar Sastry</a>,</nobr>
              
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          ACM/IEEE International Conference on Human Robot Interaction (HRI), 2020
	      <br>
       	<nobr><em style="color:orange;">Best Paper Award Winner</em></nobr>
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2020HRI-abstract" role="button" aria-expanded="false" aria-controls="bobu2020HRI-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/less/bobu2020HRI.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/view/less-human-decision-model" target="_blank">Project Website</a>
	        <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=xa_l5HeyVgw" target="_blank">Talk</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="bobu2020HRI-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
			
<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="http://www.hripioneers.info/hri20/index.html" target="_blank">
          HRI
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2020HRIpioneers" class="col p-0">
      <h5 class="title mb-0">Detecting Hypothesis Space Misspecification in Robot Learning from Human Input</h5>
      <div class="author">
                
									<nobr><em>Andreea Bobu</em></nobr>
              
      </div>

      <div>
        <p class="periodical font-italic">
          HRI Pioneers Companion of the ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2020.
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2020HRIpioneers-abstract" role="button" aria-expanded="false" aria-controls="bobu2020HRIpioneers-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2020HRIpioneers.pdf" target="_blank">PDF</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="bobu2020HRIpioneers-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Learning from human input has enabled autonomous agents to perform increasingly more complex tasks that are otherwise difficult to carry out automatically. To this end, recent works have studied how robots can incorporate such input - like demonstrations or corrections - into objective functions describing the desired behaviors. While these methods have shown progress in a variety of settings, from semi-autonomous driving, to household robotics, to automated airplane control, they all suffer from the same crucial drawback: they implicitly assume that the person's intentions can always be captured by the robot's hypothesis space. We call attention to the fact that this assumption is often unrealistic, as no model can completely account for every single possible situation ahead of time. When the robot's hypothesis space is misspecified, human input can be unhelpful - or even detrimental - to the way the robot is performing its tasks. Our work tackles this issue by proposing that the robot should first explicitly reason about how well its hypothesis space can explain human inputs, then use that situational confidence to inform how it should incorporate them.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>

			
			
			</ol>
    </div>
  </div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2019</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">

		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://www.ieee-ras.org/publications/t-ro" target="_blank">
          T-RO
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2019TRO" class="col p-0">
      <h5 class="title mb-0">Quantifying Hypothesis Space Misspecification in Learning from Human-Robot Demonstrations and Physical Corrections</h5>
      <div class="author">
                
									<nobr><em>Andreea Bobu</em>,</nobr>
              
                  <nobr><a href="https://people.eecs.berkeley.edu/~abajcsy/" target="_blank">Andrea Bajcsy<nobr><em></em></nobr></a>,</nobr>
                
                  <nobr><a href="https://ece.princeton.edu/people/jaime-fernandez-fisac" target="_blank">Jaime F. Fisac</a>,</nobr>
            
                  <nobr><a href="https://www.researchgate.net/scientific-contributions/2133993191_Sampada_Deglurkar" target="_blank">Sampada Deglurkar</a>,</nobr>
              
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          IEEE Transactions on Robotics (T-RO)
        <br>
        <nobr><em style="color:orange;">Best Paper Award Honorable Mention</em></nobr>
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2019TRO-abstract" role="button" aria-expanded="false" aria-controls="bobu2019TRO-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2019TRO.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/andreea7b/jaco_learning" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2019TRO_poster.pdf" target="_blank">Poster</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="bobu2019TRO-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Human input has enabled autonomous systems to improve their capabilities and achieve complex behaviors that are otherwise challenging to generate automatically. Recent work focuses on how robots can use such input - like demonstrations or corrections - to learn intended objectives. These techniques assume that the human’s desired objective already exists within the robot’s hypothesis space. In reality, this assumption is often inaccurate: there will always be situations where the person might care about aspects of the task that the robot does not know about. Without this knowledge, the robot cannot infer the correct objective. Hence, when the robot’s hypothesis space is misspecified, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. In this paper, we posit that the robot should reason explicitly about how well it can explain human inputs given its hypothesis space and use that situational confidence to inform how it should incorporate human input. We demonstrate our method on a 7 degree-of-freedom robot manipulator in learning from two important types of human input: demonstrations of manipulation tasks, and physical corrections during the robot’s task execution.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
  </div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2018</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://sites.google.com/a/robot-learning.org/corl2017/corl2018" target="_blank">
          CoRL
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2018CoRL" class="col p-0">
      <h5 class="title mb-0">Learning under Misspecified Objective Spaces</h5>
      <div class="author">
                
									<nobr><em>Andreea Bobu</em>,</nobr>
              
                  <nobr><a href="https://people.eecs.berkeley.edu/~abajcsy/" target="_blank">Andrea Bajcsy<nobr><em></em></nobr></a>,</nobr>
                
                  <nobr><a href="https://ece.princeton.edu/people/jaime-fernandez-fisac" target="_blank">Jaime F. Fisac</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          Conference on Robot Learning (CoRL), 2018
          <br>
          <nobr><em>Invited to Special Issue</em></nobr>
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2018CoRL-abstract" role="button" aria-expanded="false" aria-controls="bobu2018CoRL-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2018CoRL.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/andreea7b/beta_adaptive_pHRI" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/stnFye8HdcU" target="_blank">Video</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2018CoRL_poster.pdf" target="_blank">Poster</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/FSsEqEJKo8A?t=6353" target="_blank">Talk</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="bobu2018CoRL-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Learning robot objective functions from human input has become increasingly important, but state-of-the-art techniques assume that the human's desired objective lies within the robot's hypothesis space. When this is not true, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. We focus specifically on learning from physical human corrections during the robot's task execution, where not having a rich enough hypothesis space leads to the robot updating its objective in ways that the person did not actually intend. We observe that such corrections appear irrelevant to the robot, because they are not the best way of achieving any of the candidate objectives. Instead of naively trusting and learning from every human interaction, we propose robots learn conservatively by reasoning in real time about how relevant the human's correction is for the robot's hypothesis space. We test our inference method in an experiment with human interaction data, and demonstrate that this alleviates unintended learning in an in-person user study with a 7DoF robot manipulator.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>


<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://iclr.cc/Conferences/2018" target="_blank">
          ICLR
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2018ICLR" class="col p-0">
      <h5 class="title mb-0">Adapting to Continuously Shifting Domains</h5>
      <div class="author">
                
									<nobr><em>Andreea Bobu</em>,</nobr>
              
                  <nobr><a href="https://scholar.google.com/citations?user=nABXo3sAAAAJ&hl=en" target="_blank">Eric Tzeng<nobr><em></em></nobr></a>,</nobr>
                
                  <nobr><a href="https://www.cc.gatech.edu/~judy/" target="_blank">Judy Hoffman</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          International Conference on Learning Representations (ICLR) Workshop, 2018
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2018ICLR-abstract" role="button" aria-expanded="false" aria-controls="bobu2018ICLR-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/CUA/bobu2018ICLR.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/CUA/bobu2018ICLR_poster.pdf" target="_blank">Poster</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="bobu2018ICLR-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Domain adaptation typically focuses on adapting a model from a single source domain to a target domain. However, in practice, this paradigm of adapting from one source to one target is limiting, as different aspects of the real world such as illumination and weather conditions vary continuously and cannot be effectively captured by two static domains. Approaches that attempt to tackle this problem by adapting from a single source to many different target domains simultaneously are consistently unable to learn across all domain shifts. Instead, we propose an adaptation method that exploits the continuity between gradually varying domains by adapting in sequence from the source to the most similar target domain. By incrementally adapting while simultaneously efficiently regularizing against prior examples, we obtain a single strong model capable of recognition within all observed domains. Our method is applicable on a wide variety of learning settings, including visual classification and reinforcement learning in a video game domain.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>


			</ol>
    </div>
  </div>




<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2016</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
			
		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://www.miccai2016.org/en/" target="_blank">
          MICCAI
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="dalca2016MICCAI" class="col p-0">
      <h5 class="title mb-0">Patch-Based Discrete Registration of Clinical Brain Images</h5>
      <div class="author">

                  <nobr><a href="http://www.mit.edu/~adalca/" target="_blank">Adrian Dalca<nobr><em></em></nobr></a>,</nobr>
                
									<nobr><em>Andreea Bobu</em>,</nobr>
              
                  <nobr><a href="https://www.massgeneral.org/doctors/17477/natalia-rost" target="_blank">Natalia S. Rost</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.csail.mit.edu/polina/" target="_blank">Polina Golland</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
	Patch-based Techniques in Medical Imaging at the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI-PATCHMI), 2016
	<br>
	<nobr><em style="color:orange;">Best Paper Award Winner</em></nobr>
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#dalca2016MICCAI-abstract" role="button" aria-expanded="false" aria-controls="dalca2016MICCAI-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/PBR/dalca2016MICCAI.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/adalca/patchRegistration" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/PBR/dalca2016MICCAI_poster.pdf" target="_blank">Poster</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="dalca2016MICCAI-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
						We introduce a method for registration of brain images acquired in clinical settings. The algorithm relies on three-dimensional patches in a discrete registration framework to estimate correspondences. Clinical images present significant challenges for computational analysis. Fast acquisition often results in images with sparse slices, severe artifacts, and variable fields of view. Yet, large clinical datasets hold a wealth of clinically relevant information. Despite significant progress in image registration, most algorithms make strong assumptions about the continuity of image data, failing when presented with clinical images that violate these assumptions. In this paper, we demonstrate a non-rigid registration method for aligning such images. The method explicitly models the sparsely available image information to achieve robust registration. We demonstrate the algorithm on clinical images of stroke patients. The proposed method outperforms state of the art registration algorithms and avoids catastrophic failures often caused by these images. We provide a freely available open source implementation of the algorithm.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>


			</ol>
    </div>
  </div>



  </div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2020 Andreea Bobu.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
